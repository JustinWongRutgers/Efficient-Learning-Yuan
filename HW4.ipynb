{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "#                                                                        download=True, transform = transform_train))\n",
    "\n",
    "# Testing Data preparation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "#                                                                       download=True, transform = transform_test))\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a small disagreement with outside literature and the class resources about the implementation of LeNet regarding the last convolutional layer in modified LeNet that is sometimes a fully conneceted layer in other sources. I simply implement both and see that the results are good for both implementations here. LeNet is the outside source and mLeNet is the modified LeNet found in the lecture slides. The prefix 'b' denotes a batch norm layer immediately after the activation of the disputed layer and the prefix 'd' denotes a dropout layer immediately after the activation of the disputed layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "    \n",
    "        return out\n",
    "\n",
    "class mLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mLeNet, self).__init__()\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, 5)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "    \n",
    "        return out\n",
    "\n",
    "class bLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bLeNet, self).__init__()\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        self.bn = nn.BatchNorm1d(120)\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "    \n",
    "        return out\n",
    "\n",
    "class bmLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bmLeNet, self).__init__()\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, 5)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        self.bn = nn.BatchNorm1d(120)\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "    \n",
    "        return out\n",
    "    \n",
    "class dLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dLeNet, self).__init__()\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        self.dd = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dd(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "    \n",
    "        return out\n",
    "\n",
    "class dmLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dmLeNet, self).__init__()\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, 5)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "    \n",
    "        return out\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    batches = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        ############################\n",
    "        #### Put your code here ####\n",
    "        ############################\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if batch_idx == 0:\n",
    "        #    print('Train Epoch: {}\\tLoss: {:.6f}'.format(\n",
    "        #        epoch+1, loss.item()))\n",
    "        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        batches = batch_idx + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        #### End of your codes ####\n",
    "        ###########################\n",
    "        #if batch_idx % 10 == 0:\n",
    "        #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #        100. * batch_idx / len(train_loader), loss.item()))\n",
    "    print('\\nTraining set epoch {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch,\n",
    "        train_loss/ batches, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "LeNet\n",
      "\n",
      "Training set epoch 1: Average loss: 1.8646, Accuracy: 15151/50000 (30%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.6242, Accuracy: 4038/10000 (40%)\n",
      "\n",
      "\n",
      "Training set epoch 2: Average loss: 1.5982, Accuracy: 20574/50000 (41%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.4619, Accuracy: 4621/10000 (46%)\n",
      "\n",
      "\n",
      "Training set epoch 3: Average loss: 1.5158, Accuracy: 22425/50000 (45%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.6480, Accuracy: 5123/10000 (51%)\n",
      "\n",
      "\n",
      "Training set epoch 4: Average loss: 1.4678, Accuracy: 23589/50000 (47%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9266, Accuracy: 5135/10000 (51%)\n",
      "\n",
      "\n",
      "Training set epoch 5: Average loss: 1.4367, Accuracy: 24352/50000 (49%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.5404, Accuracy: 5253/10000 (53%)\n",
      "\n",
      "\n",
      "Training set epoch 6: Average loss: 1.4012, Accuracy: 25015/50000 (50%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0673, Accuracy: 5213/10000 (52%)\n",
      "\n",
      "\n",
      "Training set epoch 7: Average loss: 1.3869, Accuracy: 25343/50000 (51%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9930, Accuracy: 5442/10000 (54%)\n",
      "\n",
      "\n",
      "Training set epoch 8: Average loss: 1.3778, Accuracy: 25619/50000 (51%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0927, Accuracy: 5497/10000 (55%)\n",
      "\n",
      "\n",
      "Training set epoch 9: Average loss: 1.3420, Accuracy: 26283/50000 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.5657, Accuracy: 5488/10000 (55%)\n",
      "\n",
      "\n",
      "Training set epoch 10: Average loss: 1.3439, Accuracy: 26288/50000 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2273, Accuracy: 5631/10000 (56%)\n",
      "\n",
      "Traning and Testing total excution time is: 895.7506258487701 seconds \n",
      "Modified LeNet\n",
      "\n",
      "Training set epoch 1: Average loss: 1.8295, Accuracy: 15998/50000 (32%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.3510, Accuracy: 4259/10000 (43%)\n",
      "\n",
      "\n",
      "Training set epoch 2: Average loss: 1.5840, Accuracy: 21136/50000 (42%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.4957, Accuracy: 4773/10000 (48%)\n",
      "\n",
      "\n",
      "Training set epoch 3: Average loss: 1.4931, Accuracy: 23023/50000 (46%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.2768, Accuracy: 4948/10000 (49%)\n",
      "\n",
      "\n",
      "Training set epoch 4: Average loss: 1.4407, Accuracy: 24319/50000 (49%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9068, Accuracy: 5339/10000 (53%)\n",
      "\n",
      "\n",
      "Training set epoch 5: Average loss: 1.4367, Accuracy: 24352/50000 (49%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.6699, Accuracy: 5288/10000 (53%)\n",
      "\n",
      "\n",
      "Training set epoch 6: Average loss: 1.3997, Accuracy: 25033/50000 (50%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2348, Accuracy: 5447/10000 (54%)\n",
      "\n",
      "\n",
      "Training set epoch 7: Average loss: 1.3749, Accuracy: 25834/50000 (52%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9728, Accuracy: 5378/10000 (54%)\n",
      "\n",
      "\n",
      "Training set epoch 8: Average loss: 1.3533, Accuracy: 26149/50000 (52%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.5803, Accuracy: 5446/10000 (54%)\n",
      "\n",
      "\n",
      "Training set epoch 9: Average loss: 1.3574, Accuracy: 26077/50000 (52%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.7333, Accuracy: 5536/10000 (55%)\n",
      "\n",
      "\n",
      "Training set epoch 10: Average loss: 1.3474, Accuracy: 26227/50000 (52%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.1402, Accuracy: 5637/10000 (56%)\n",
      "\n",
      "Traning and Testing total excution time is: 958.4536871910095 seconds \n",
      "LeNet with batch normalization layer\n",
      "\n",
      "Training set epoch 1: Average loss: 1.6772, Accuracy: 18877/50000 (38%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.3290, Accuracy: 4628/10000 (46%)\n",
      "\n",
      "\n",
      "Training set epoch 2: Average loss: 1.4596, Accuracy: 23380/50000 (47%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.2232, Accuracy: 4456/10000 (45%)\n",
      "\n",
      "\n",
      "Training set epoch 3: Average loss: 1.3810, Accuracy: 24948/50000 (50%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.6263, Accuracy: 5155/10000 (52%)\n",
      "\n",
      "\n",
      "Training set epoch 4: Average loss: 1.3035, Accuracy: 26582/50000 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0303, Accuracy: 5736/10000 (57%)\n",
      "\n",
      "\n",
      "Training set epoch 5: Average loss: 1.2622, Accuracy: 27378/50000 (55%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.1789, Accuracy: 5335/10000 (53%)\n",
      "\n",
      "\n",
      "Training set epoch 6: Average loss: 1.2338, Accuracy: 27824/50000 (56%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2434, Accuracy: 5879/10000 (59%)\n",
      "\n",
      "\n",
      "Training set epoch 7: Average loss: 1.2076, Accuracy: 28346/50000 (57%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.0247, Accuracy: 5683/10000 (57%)\n",
      "\n",
      "\n",
      "Training set epoch 8: Average loss: 1.1974, Accuracy: 28516/50000 (57%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.1002, Accuracy: 5592/10000 (56%)\n",
      "\n",
      "\n",
      "Training set epoch 9: Average loss: 1.1837, Accuracy: 28923/50000 (58%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.4884, Accuracy: 6108/10000 (61%)\n",
      "\n",
      "\n",
      "Training set epoch 10: Average loss: 1.1589, Accuracy: 29271/50000 (59%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2729, Accuracy: 5382/10000 (54%)\n",
      "\n",
      "Traning and Testing total excution time is: 913.3372478485107 seconds \n",
      "Modified LeNet with batch normalization layer\n",
      "\n",
      "Training set epoch 1: Average loss: 1.6810, Accuracy: 18556/50000 (37%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.2874, Accuracy: 4741/10000 (47%)\n",
      "\n",
      "\n",
      "Training set epoch 2: Average loss: 1.4560, Accuracy: 23419/50000 (47%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.5841, Accuracy: 5320/10000 (53%)\n",
      "\n",
      "\n",
      "Training set epoch 3: Average loss: 1.3499, Accuracy: 25580/50000 (51%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9890, Accuracy: 5414/10000 (54%)\n",
      "\n",
      "\n",
      "Training set epoch 4: Average loss: 1.2749, Accuracy: 27222/50000 (54%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2899, Accuracy: 5510/10000 (55%)\n",
      "\n",
      "\n",
      "Training set epoch 5: Average loss: 1.2310, Accuracy: 27860/50000 (56%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2694, Accuracy: 5725/10000 (57%)\n",
      "\n",
      "\n",
      "Training set epoch 6: Average loss: 1.2273, Accuracy: 28058/50000 (56%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.1911, Accuracy: 4448/10000 (44%)\n",
      "\n",
      "\n",
      "Training set epoch 7: Average loss: 1.1898, Accuracy: 28666/50000 (57%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.5314, Accuracy: 6039/10000 (60%)\n",
      "\n",
      "\n",
      "Training set epoch 8: Average loss: 1.1581, Accuracy: 29209/50000 (58%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.4627, Accuracy: 6196/10000 (62%)\n",
      "\n",
      "\n",
      "Training set epoch 9: Average loss: 1.1470, Accuracy: 29459/50000 (59%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.5357, Accuracy: 5993/10000 (60%)\n",
      "\n",
      "\n",
      "Training set epoch 10: Average loss: 1.1332, Accuracy: 29746/50000 (59%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.5338, Accuracy: 6120/10000 (61%)\n",
      "\n",
      "Traning and Testing total excution time is: 982.1303346157074 seconds \n",
      "LeNet with dropout layer\n",
      "\n",
      "Training set epoch 1: Average loss: 1.9489, Accuracy: 13177/50000 (26%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.9531, Accuracy: 3871/10000 (39%)\n",
      "\n",
      "\n",
      "Training set epoch 2: Average loss: 1.7195, Accuracy: 18070/50000 (36%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.8107, Accuracy: 4451/10000 (45%)\n",
      "\n",
      "\n",
      "Training set epoch 3: Average loss: 1.6552, Accuracy: 19563/50000 (39%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.4647, Accuracy: 4368/10000 (44%)\n",
      "\n",
      "\n",
      "Training set epoch 4: Average loss: 1.6136, Accuracy: 20470/50000 (41%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.0018, Accuracy: 4627/10000 (46%)\n",
      "\n",
      "\n",
      "Training set epoch 5: Average loss: 1.5948, Accuracy: 20974/50000 (42%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.1812, Accuracy: 4894/10000 (49%)\n",
      "\n",
      "\n",
      "Training set epoch 6: Average loss: 1.5509, Accuracy: 21869/50000 (44%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.0400, Accuracy: 4519/10000 (45%)\n",
      "\n",
      "\n",
      "Training set epoch 7: Average loss: 1.5626, Accuracy: 21865/50000 (44%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.0060, Accuracy: 4777/10000 (48%)\n",
      "\n",
      "\n",
      "Training set epoch 8: Average loss: 1.5425, Accuracy: 22166/50000 (44%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.3426, Accuracy: 5070/10000 (51%)\n",
      "\n",
      "\n",
      "Training set epoch 9: Average loss: 1.5354, Accuracy: 22449/50000 (45%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.2013, Accuracy: 4884/10000 (49%)\n",
      "\n",
      "\n",
      "Training set epoch 10: Average loss: 1.5303, Accuracy: 22704/50000 (45%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.1355, Accuracy: 5158/10000 (52%)\n",
      "\n",
      "Traning and Testing total excution time is: 890.8146650791168 seconds \n",
      "Modified LeNet with dropout layer\n",
      "\n",
      "Training set epoch 1: Average loss: 1.8682, Accuracy: 14927/50000 (30%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -1.8641, Accuracy: 4274/10000 (43%)\n",
      "\n",
      "\n",
      "Training set epoch 2: Average loss: 1.6331, Accuracy: 20031/50000 (40%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.0441, Accuracy: 4731/10000 (47%)\n",
      "\n",
      "\n",
      "Training set epoch 3: Average loss: 1.5265, Accuracy: 22419/50000 (45%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.5896, Accuracy: 4624/10000 (46%)\n",
      "\n",
      "\n",
      "Training set epoch 4: Average loss: 1.4717, Accuracy: 23682/50000 (47%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.8459, Accuracy: 5100/10000 (51%)\n",
      "\n",
      "\n",
      "Training set epoch 5: Average loss: 1.4085, Accuracy: 24945/50000 (50%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.2489, Accuracy: 5500/10000 (55%)\n",
      "\n",
      "\n",
      "Training set epoch 6: Average loss: 1.4010, Accuracy: 25167/50000 (50%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.8389, Accuracy: 5535/10000 (55%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set epoch 7: Average loss: 1.3761, Accuracy: 25990/50000 (52%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.6460, Accuracy: 5283/10000 (53%)\n",
      "\n",
      "\n",
      "Training set epoch 8: Average loss: 1.3526, Accuracy: 26284/50000 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.7049, Accuracy: 5468/10000 (55%)\n",
      "\n",
      "\n",
      "Training set epoch 9: Average loss: 1.3449, Accuracy: 26563/50000 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -3.3513, Accuracy: 5668/10000 (57%)\n",
      "\n",
      "\n",
      "Training set epoch 10: Average loss: 1.3058, Accuracy: 27185/50000 (54%)\n",
      "\n",
      "\n",
      "Test set: Average loss: -2.9500, Accuracy: 5879/10000 (59%)\n",
      "\n",
      "Traning and Testing total excution time is: 956.0515804290771 seconds \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    # Training settings\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "    lr = 0.05\n",
    "    no_cuda = True\n",
    "    save_model = False\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(100)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "    \n",
    "    \n",
    "    def model_train(network, name):\n",
    "        model = network().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        print(name)\n",
    "        time0 = time.time()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train( model, device, train_loader, optimizer, epoch)\n",
    "            test( model, device, test_loader)\n",
    "        time1 = time.time() \n",
    "        \n",
    "        print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
    "        \n",
    "    model_train(LeNet, 'LeNet')\n",
    "    model_train(mLeNet, 'Modified LeNet')\n",
    "    model_train(bLeNet, 'LeNet with batch normalization layer')\n",
    "    model_train(bmLeNet, 'Modified LeNet with batch normalization layer')\n",
    "    model_train(dLeNet, 'LeNet with dropout layer')\n",
    "    model_train(dmLeNet, 'Modified LeNet with dropout layer')\n",
    "    #model = dLeNet().to(device)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
